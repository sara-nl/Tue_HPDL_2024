#!/bin/bash
#SBATCH --job-name=mnist-array
#SBATCH --partition=gpu
#SBATCH --time=10:00
#SBATCH --nodes=1
#SBATCH --ntasks=1
#SBATCH --cpus-per-task=1
#SBATCH --array=1-9
#SBATCH --gpus-per-node=1


module load 2022
module load PyTorch/1.12.0-foss-2022a-CUDA-11.7.0
module load torchvision/0.13.1-foss-2022a-CUDA-11.7.0

export MASTER_PORT=$(expr 10000 + $(echo -n $SLURM_JOBID | tail -c 4))
master_addr=$(scontrol show hostnames "$SLURM_JOB_NODELIST" | head -n 1)
export MASTER_ADDR=$master_addr

# minst_classify_ddp.py expects TEACHER_DIR to be set in order to find the data
export TEACHER_DIR=/projects/0/jhssrf004

# Specify the path to the config file
config=$HOME/JHS_notebooks/MLonHPC_2day_Okt2023/Day2/notebooks/array_config.txt

# Extract the batch_size value for the current $SLURM_ARRAY_TASK_ID
batch_size=$(awk -v ArrayTaskID=$SLURM_ARRAY_TASK_ID '$1==ArrayTaskID {print $2}' $config)

# Extract the layer_size value for the current $SLURM_ARRAY_TASK_ID
learning_rate=$(awk -v ArrayTaskID=$SLURM_ARRAY_TASK_ID '$1==ArrayTaskID {print $3}' $config)

# Print to a file a message that includes the current $SLURM_ARRAY_TASK_ID, batch_size, and layer_size
echo "This is array task ${SLURM_ARRAY_TASK_ID}, the batch_size is ${batch_size} and the learning_rate is ${learning_rate}."

python mnist_classify_ddp.py --batch-size=${batch_size} --lr=${learning_rate}

# User Knowledge Base page about array jobs
# https://servicedesk.surf.nl/wiki/display/WIKI/Array+jobs
